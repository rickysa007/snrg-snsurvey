{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline, interp1d, splrep\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel\n",
    "import george\n",
    "from george import kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the .json file\n",
    "\n",
    "os.chdir(r\"C:\\Users\\ricky\\JupyterNotebooks\\Intern21\\import_photometry_data\\selected_all_type_photometry\")\n",
    "filename = glob.glob('*.json')\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(filename)\n",
    "print(len(filename))\n",
    "print(filename)\n",
    "\n",
    "# Create a list for all .json, the 1st SN saved as json_data[0], the 2nd SN saved as json_data[1], etc.\n",
    "json_data = []\n",
    "for i in filename:\n",
    "    with open(i, encoding=\"utf-8\") as f:\n",
    "        json_data.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e21b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avoid_emptySN(filename, Filter):\n",
    "    \n",
    "    Band = []\n",
    "    filename_id = []\n",
    "    #filename1 = filename\n",
    "    #print(filename)\n",
    "    for i in range(len(filename)):\n",
    "        #print(i)\n",
    "        Band.append([])\n",
    "        \n",
    "        SN_name = filename[i].replace('.json', '')\n",
    "        SN_name = SN_name.replace('_', ':')\n",
    "        \n",
    "        N = len(json_data[i][SN_name]['photometry']) # The no. of data point of photometry in each SN\n",
    "        #print(SN_name)\n",
    "        for j in range(N): # Loop through all photemetry datapoint in one SN\n",
    "            # Avoid any data point without band data\n",
    "            try:\n",
    "                Band[i].append(json_data[i][SN_name]['photometry'][j]['band'])\n",
    "            except:\n",
    "                Band[i].append(0)\n",
    "                \n",
    "        a = 0\n",
    "        for j in range(N):\n",
    "            for k in range(len(Filter)):\n",
    "                if Band[i][j] == Filter[k]:\n",
    "                    a += 1\n",
    "        \n",
    "        if a > 10: # Any SN with more than 10 data points will be used\n",
    "            filename_id.append(i)\n",
    "            \n",
    "    return filename_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filter_JMC = ['U', 'B', 'V', 'R', 'I'] # the Johnson-Morgon-Cousins UBVRI filter system\n",
    "Filter_SDSS_p = [\"u'\", \"g'\", \"r'\", \"i'\"] # the SDSS primed filter system\n",
    "Filter_SDSS = ['u', 'g', 'r', 'i'] # the SDSS filter system\n",
    "# For now the primed and unprimed SDSS filter will be treated same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_JMC = avoid_emptySN(filename, Filter_JMC)\n",
    "filename_SDSS_p = avoid_emptySN(filename, Filter_SDSS_p)\n",
    "filename_SDSS = avoid_emptySN(filename, Filter_SDSS)\n",
    "\n",
    "print(len(filename_JMC), len(filename_SDSS_p), len(filename_SDSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52528011",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename[filename_JMC[245]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af38dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Extract data from .json\n",
    "\n",
    "Input\n",
    "    Filter: list of string\n",
    "        1D list\n",
    "        Name of each band of a particular filter system in use\n",
    "    \n",
    "Return\n",
    "    Time: list \n",
    "        shape=(len(Filter), len(filename), number of data)\n",
    "        Time of datapoints of SN\n",
    "        \n",
    "    Magnitude_Abs: list\n",
    "        shape=(len(Filter), len(filename), number of data)\n",
    "        Absolute magnitude of datapoints of SN,\n",
    "        calculated from the relative magnitude, redshift, and luminosity distance\n",
    "        \n",
    "    Magnitude_Abs_err: list\n",
    "        shape=(len(Filter), len(filename), number of data)\n",
    "        Error of absolute magnitude of datapoints of SN,\n",
    "        calculated from relative magnitude error and luminosity distance error\n",
    "        \n",
    "    Type: list\n",
    "        1D list\n",
    "        Store the claimed type of the SN\n",
    "'''\n",
    "\n",
    "def lc_extractor(filename, filename_id, Filter):\n",
    "    \n",
    "    Band = [] # Contain EM band chosen for analysis\n",
    "    Type = [] # Claimed type from the .json\n",
    "    \n",
    "    Time = [] # Contain time (MJD) for each band\n",
    "    Magnitude_Abs = [] # Contain absolute magnitude for each band\n",
    "    Magnitude_Abs_err = [] # Contain error of absolute magnitude for each band\n",
    "    \n",
    "    for j in range(len(Filter)): # Create a dimension to store the data at each band separately\n",
    "        Time.append([]) \n",
    "        Magnitude_Abs.append([])\n",
    "        Magnitude_Abs_err.append([])\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(len(filename_id)): # Loop through all SN\n",
    "\n",
    "        Band.append([]) # Create 2D list\n",
    "\n",
    "        SN_name = filename[filename_id[i]].replace('.json', '')\n",
    "        SN_name = SN_name.replace('_', ':')\n",
    "        \n",
    "        LumDist = float(json_data[filename_id[i]][SN_name]['lumdist'][0]['value']) # Obtain the luminosity distance\n",
    "        \n",
    "        try:\n",
    "            LumDist_err = float(json_data[filename_id[i]][SN_name]['lumdist'][0]['e_value'])\n",
    "        except:\n",
    "            LumDist_err = 0\n",
    "\n",
    "        z = float(json_data[filename_id[i]][SN_name]['redshift'][0]['value']) # Obtain the redshift, z\n",
    "\n",
    "        N = len(json_data[filename_id[i]][SN_name]['photometry']) # The no. of data point of photometry in each SN\n",
    "\n",
    "        Type.append(json_data[filename_id[i]][SN_name]['claimedtype'][0]['value']) \n",
    "\n",
    "        for j in range(len(Filter)):\n",
    "\n",
    "            Time[j].append([])\n",
    "            Magnitude_Abs[j].append([])\n",
    "            Magnitude_Abs_err[j].append([])\n",
    "\n",
    "        for j in range(N): # Loop through all photemetry datapoint in one SN\n",
    "            # Avoid any data point without band data\n",
    "            try:\n",
    "                Band[i].append(json_data[filename_id[i]][SN_name]['photometry'][j]['band'])\n",
    "            except:\n",
    "                Band[i].append(0)\n",
    "\n",
    "            for k in range(len(Filter)):\n",
    "                # Create light curves for every sources   \n",
    "                if Band[i][j] == Filter[k]:\n",
    "\n",
    "                    Magnitude_App = float(json_data[filename_id[i]][SN_name]['photometry'][j]['magnitude']) # Obtain the apparent magnitude from photometry       \n",
    "\n",
    "                    Time[k][i].append(float(json_data[filename_id[i]][SN_name]['photometry'][j]['time'])) # Fill the Time list\n",
    "                    Magnitude_Abs[k][i].append(Magnitude_App - 5*np.log10(LumDist*1e5) + 2.5*np.log10(1+z)) # Calculate the absolute magnitude and fill the Magnitude_Abs list\n",
    "\n",
    "                    try:\n",
    "                        Magnitude_App_err = float(json_data[filename_id[i]][SN_name]['photometry'][j]['e_magnitude'])\n",
    "                        Magnitude_Abs_err[k][i].append(np.sqrt(Magnitude_App_err**2 + 5*0.434*LumDist_err/LumDist))\n",
    "                    except:\n",
    "                        Magnitude_Abs_err[k][i].append(0.3)\n",
    "            \n",
    "            \n",
    "    return Time, Magnitude_Abs, Magnitude_Abs_err, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbbed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_JMC, Magnitude_Abs_JMC, Magnitude_Abs_err_JMC, Type_JMC = lc_extractor(filename, filename_JMC, Filter_JMC)\n",
    "Time_SDSS_p, Magnitude_Abs_SDSS_p, Magnitude_Abs_err_SDSS_p, Type_SDSS_p = lc_extractor(filename, filename_SDSS_p, Filter_SDSS_p)\n",
    "Time_SDSS, Magnitude_Abs_SDSS, Magnitude_Abs_err_SDSS, Type_SDSS = lc_extractor(filename, filename_SDSS, Filter_SDSS)\n",
    "\n",
    "print(np.array(Time_JMC).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a894d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data lists into shape=(len(filename_id), len(Filter), data points)\n",
    "def transpose(Time, Magnitude_Abs, Magnitude_Abs_err):\n",
    "    return np.array(Time, dtype=\"object\").T, np.array(Magnitude_Abs, dtype=\"object\").T, np.array(Magnitude_Abs_err, dtype=\"object\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4da9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_JMC, m_JMC, m_err_JMC = transpose(Time_JMC, Magnitude_Abs_JMC, Magnitude_Abs_err_JMC)\n",
    "t_SDSS_p, m_SDSS_p, m_err_SDSS_p = transpose(Time_SDSS_p, Magnitude_Abs_SDSS_p, Magnitude_Abs_err_SDSS_p)\n",
    "t_SDSS, m_SDSS, m_err_SDSS = transpose(Time_SDSS, Magnitude_Abs_SDSS, Magnitude_Abs_err_SDSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Zeroing the time list by the peak magnitude such that the peak has day = 0\n",
    "Also truncate the data lists to only include -50 days to 135 days post peak'''\n",
    "def peak_zeroing(t, m, m_err, Filter, lightcurve_length_before, lightcurve_length_after):\n",
    "    \n",
    "    for i in range(len(t)):\n",
    "        # Find the maximum time in shortest wavelength band\n",
    "        maximum = 0\n",
    "        maximum1 = []\n",
    "        a = None\n",
    "\n",
    "        if len(m[i][1]) != 0: # choosing B band for maximum calculation as default\n",
    "            a = 1 \n",
    "        else:\n",
    "            for j in range(len(Filter)): # choose other bands if B band is empty\n",
    "                if len(m[i][j]) != 0:\n",
    "                    a = j\n",
    "                    break\n",
    "\n",
    "        maximum = np.argmin(m[i][a]) # save the id of maximum magnitude, in B band or otherwise\n",
    "        t_max = t[i][a][maximum] # save the time of maximum magnitude\n",
    "\n",
    "        for j in range(len(Filter)):\n",
    "\n",
    "            t[i][j] = np.array(t[i][j]) - t_max # shift the days of time list such that the peak is 0 days\n",
    "\n",
    "            t[i][j] = np.delete(t[i][j], np.where(t[i][j] > lightcurve_length_after)) # truncate away any time data after 135 days\n",
    "            m[i][j] = m[i][j][0:len(t[i][j])] # truncate away any magnitude after 135 days\n",
    "            m_err[i][j] = m_err[i][j][0:len(t[i][j])] # truncate away any magnitude error after 135 days\n",
    "\n",
    "            t[i][j] = np.delete(t[i][j], np.where(t[i][j] < lightcurve_length_before)) # truncate away any time data before -50 days\n",
    "            m[i][j] = m[i][j][len(m[i][j]) - len(t[i][j]):] # truncate away any magnitude data before -50 days\n",
    "            m_err[i][j] = m_err[i][j][len(m_err[i][j]) - len(t[i][j]):] # truncate away any magnitude error data before -50 days\n",
    "\n",
    "            if (len(t[i][j]) - len(m_err[i][j])) !=0:\n",
    "                print('bruh') # print bruh if the length of the lists doesn't match\n",
    "\n",
    "    return t, m, m_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fdbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve_length_before = -50\n",
    "lightcurve_length_after = 135\n",
    "\n",
    "t_JMC, m_JMC, m_err_JMC = peak_zeroing(t_JMC, m_JMC, m_err_JMC, Filter_JMC, lightcurve_length_before, lightcurve_length_after)\n",
    "t_SDSS_p, m_SDSS_p, m_err_SDSS_p = peak_zeroing(t_SDSS_p, m_SDSS_p, m_err_SDSS_p, Filter_SDSS_p, lightcurve_length_before, lightcurve_length_after)\n",
    "t_SDSS, m_SDSS, m_err_SDSS = peak_zeroing(t_SDSS, m_SDSS, m_err_SDSS, Filter_SDSS, lightcurve_length_before, lightcurve_length_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c359bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(m_JMC[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309dea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP_normalization(y, y_err):\n",
    "    return (y - np.mean(y)) / (np.max(y) - np.min(y)), y_err / (np.max(y) - np.min(y)), np.mean(y), np.max(y) - np.min(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137460cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP_interpolate(t, m, m_err, Type, Filter, Filter_wavelength, filename1, color):\n",
    "    \n",
    "    data = []\n",
    "    padding_point = []\n",
    "    \n",
    "    data_mean = []\n",
    "    data_range = []\n",
    "    \n",
    "    m1 = []\n",
    "    m1_err = []\n",
    "    \n",
    "    selected = [180]\n",
    "    \n",
    "    #for i in selected:\n",
    "    for i in range(len(t)):\n",
    "        Filter_num = [] # List containing Filter_wavelength for GP fitting, with number of element matching the number of element in x (time)\n",
    "        x = [] # List of t containing all bands\n",
    "        y = [] # List of m containing all bands\n",
    "        y_err = [] # List of m_err containing all bands\n",
    "        residual = [] # List of GP prediction - actual observation\n",
    "        a = 'empty' # Store the band number of the longest band timewise\n",
    "        b = 0 # Store the time duration of the longest band timewise\n",
    "        \n",
    "        m1.append([])\n",
    "        m1_err.append([])\n",
    "        for j in range(len(Filter)):\n",
    "            m1[i].append([])\n",
    "            m1_err[i].append([])\n",
    "        \n",
    "        for j in range(len(Filter)):\n",
    "            # Finding which band has the longest light curve\n",
    "            if len(t[i][j]) != 0:\n",
    "                if (t[i][j][-1] - t[i][j][0]) > b:\n",
    "                    b = t[i][j][-1] - t[i][j][0]\n",
    "                    a = j\n",
    "\n",
    "            # Saving the t and m and m_err for all bands for GP fitting\n",
    "            for k in range(len(t[i][j])):\n",
    "                x.append(t[i][j][k])\n",
    "                y.append(m[i][j][k])\n",
    "                y_err.append(m_err[i][j][k])\n",
    "                Filter_num.append(Filter_wavelength[j])\n",
    "\n",
    "        x1 = np.vstack([x, Filter_num]).T # 2D list with both the time and Filter_wavelength used for initial GP fitting\n",
    "\n",
    "\n",
    "        Filter_pred = [] # List containing Filter_wavelength for GP fitting, with number of element matching the number of element in x (time) \n",
    "        x_pred = [] # List of t containing all bands\n",
    "        for j in range(len(Filter)):\n",
    "            x_pred_temp = np.linspace(t[i][a][0], t[i][a][-1], 500)\n",
    "            for k in range(500):\n",
    "                x_pred.append(x_pred_temp[k])\n",
    "                Filter_pred.append(Filter_wavelength[j])\n",
    "\n",
    "\n",
    "        x_pred1 = np.vstack([x_pred, Filter_pred]).T # 2D list with both the artificial time list and Filter_wavelength used for GP generation\n",
    "        \n",
    "        \n",
    "        # Normalization\n",
    "        y, y_err, y_mean, y_range = GP_normalization(y, y_err) # Normalizing the magnitude and magnitude error such that the GP mean is 0\n",
    "        \n",
    "        for j in range(len(Filter)):\n",
    "            m1[i][j] = (m[i][j] - y_mean) / y_range\n",
    "            m1_err[i][j] = m_err[i][j] / y_range\n",
    "        \n",
    "        data_mean.append(y_mean)\n",
    "        data_range.append(y_range)\n",
    "        \n",
    "        \n",
    "        \n",
    "        kernel = np.var(y)*kernels.Matern32Kernel(metric=[100,1], ndim=2) + kernels.ConstantKernel(log_constant=0, ndim=2) # 3/2 Matern Kernel and Constant Kernel\n",
    "        gp = george.GP(kernel)\n",
    "        \n",
    "        gp.compute(x1, y_err) # Initial GP fitting\n",
    "\n",
    "        def neg_ln_like(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            return -gp.log_likelihood(np.array(y))\n",
    "\n",
    "        def grad_neg_ln_like(p):\n",
    "            gp.set_parameter_vector(p)\n",
    "            return -gp.grad_log_likelihood(np.array(y))\n",
    "\n",
    "        try:\n",
    "            result = minimize(neg_ln_like, gp.get_parameter_vector(), jac=grad_neg_ln_like, method='L-BFGS-B') # Fitting for finding the optimum parameters of the kernel\n",
    "            print(i, result)\n",
    "            print(i, 'converged successfully')\n",
    "            gp.recompute()\n",
    "            pred, pred_var = gp.predict(y, x_pred1, return_var=True) # GP generation with artifical time\n",
    "        except:\n",
    "            print(i, 'failed to converge')\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Plot out the result of the GP generation with artificial time\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(35, 9))\n",
    "        #plt.gca().invert_yaxis()\n",
    "        for j in range(len(Filter)):\n",
    "            for k in range(2):\n",
    "                axs[k].plot(x_pred[j*500:(j+1)*500], pred[j*500:(j+1)*500], color=color[j], lw=1.5, alpha=0.8, label=Filter[j])\n",
    "                axs[k].fill_between(x_pred[j*500:(j+1)*500], pred[j*500:(j+1)*500] - np.sqrt(pred_var[j*500:(j+1)*500]), pred[j*500:(j+1)*500] + np.sqrt(pred_var[j*500:(j+1)*500]),\n",
    "                                color=color[j], alpha=0.2)\n",
    "                axs[k].errorbar(t[i][j], m1[i][j], yerr=m1_err[i][j], fmt='.', color=color[j], capsize=0)\n",
    "                axs[0].set_xlim(t[i][a][0], t[i][a][-1])\n",
    "                axs[1].set_xlim(-50, 135)\n",
    "                axs[1].set_ylim(min(pred[j*500:(j+1)*500])-0.5, max(pred[j*500:(j+1)*500])+0.5)\n",
    "        \n",
    "        for k in range(2):\n",
    "            axs[k].invert_yaxis()\n",
    "            axs[k].grid()\n",
    "            axs[k].legend()\n",
    "            #axs[k].set_title(filename[filename1[i]], color='white')\n",
    "            axs[k].set_title(filename[filename1[i]]+' '+Type[i], color='white')\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        pred_residual, pred_var_residual = gp.predict(y, x1, return_var=True) # GP generation with actual observation\n",
    "        #print('shape of pred_residual', pred_residual.shape)\n",
    "\n",
    "\n",
    "        fig, axs = plt.subplots(1, len(Filter)+1, figsize=(35, 6))\n",
    "        l = 0\n",
    "\n",
    "        # Calculate the residual\n",
    "        for j in range(len(Filter)+1):\n",
    "            residual.append([])\n",
    "            if j != len(Filter): # Residual calculation for each single band\n",
    "                for k in range(len(m1[i][j])):\n",
    "                    residual[j].append(-1*(m1[i][j][k] - pred_residual[l+k])) # Notice the sign is flipped for easier observation for our ape brain\n",
    "                l += len(m1[i][j]) # id calculation as pred_residual is just a 1D long list containing all bands\n",
    "            else:\n",
    "                for k in range(len(y)): # Residual calculation for all bands\n",
    "                    residual[j].append(-1*(y[k] - pred_residual[k])) # Again the sign is flipped\n",
    "\n",
    "            mu, sigma = norm.fit(residual[j]) # Fit a Gaussian distribution on the residual\n",
    "\n",
    "            # Plotting the residual\n",
    "            values,bins,_ = axs[j].hist(residual[j], bins=20, color=color[j], density=False, alpha=0.7)\n",
    "            area = sum(np.diff(bins)*values)\n",
    "\n",
    "            if len(residual[j]) == 0: # Skip empty bands\n",
    "                continue\n",
    "\n",
    "            gcxmin = np.min(residual[j])\n",
    "            gcxmax = np.max(residual[j])\n",
    "            gcx = np.linspace(gcxmin, gcxmax, 100)\n",
    "            gc = area*norm.pdf(gcx, mu, sigma)\n",
    "\n",
    "            axs[j].plot(gcx, gc, color=color[j], linestyle='--', linewidth=2)\n",
    "            axs[j].axvline(x=0, color='k', label='0')\n",
    "            axs[j].axvline(x=mu, color=color[j], linestyle='--', label='mean')\n",
    "            axs[j].legend()\n",
    "            axs[j].grid()\n",
    "            axs[j].set_title('mean = '+'%.3f'%mu+', var = '+'%.3f'%sigma, color='white')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # Temporary variable per SN\n",
    "        data_t = [] # Store the time from GP interpolation\n",
    "        data_m = [] # Store the magnitude from GP interpolation\n",
    "        data_m_err = [] # Store the error of magnitude from GP interpolation\n",
    "        data_temp = [] # Store all temporary data per SN, this list will be saved to the actual data[] within every loop\n",
    "\n",
    "        Filter_pred = []\n",
    "        data_t_temp = np.linspace(t[i][a][0], t[i][a][-1], int(t[i][a][-1] - t[i][a][0])+1) # Time list for GP interpolation, data points with 1 day apart\n",
    "        padding_point.append(len(data_t_temp)) # Padding was used after the length of the GP interpolation\n",
    "        \n",
    "        # Initialize time list for GP interpolation\n",
    "        for j in range(len(Filter)):   \n",
    "            for k in range(len(data_t_temp)):\n",
    "                data_t.append(data_t_temp[k])\n",
    "                Filter_pred.append(Filter_wavelength[j])\n",
    "\n",
    "        data_t1 = np.vstack([data_t, Filter_pred]).T\n",
    "        data_m_temp, data_m_err_temp = gp.predict(y, data_t1, return_var=True)\n",
    "\n",
    "        lightcurve_length = np.arange(lightcurve_length_before, lightcurve_length_after, 1) # The whole length of time list\n",
    "        \n",
    "        # LC padding\n",
    "        for j in range(len(Filter)):\n",
    "            data_m.append([])\n",
    "            data_m_err.append([])\n",
    "            for k in lightcurve_length:\n",
    "                if k < t[i][a][0]:\n",
    "                    pass\n",
    "                elif k > t[i][a][-1]:\n",
    "                    #print((j+1)*(len(data_t_temp))-1)\n",
    "                    data_m[j].append(data_m_temp[(j+1)*(len(data_t_temp))-1])\n",
    "                    data_m_err[j].append(np.sqrt(data_m_err_temp[(j+1)*(len(data_t_temp))-1]))\n",
    "                else:\n",
    "                    #print(j*(len(data_t_temp))+k-int(t[i][a][0]))\n",
    "                    data_m[j].append(data_m_temp[j*(len(data_t_temp))+k-int(t[i][a][0])])\n",
    "                    data_m_err[j].append(np.sqrt(data_m_err_temp[j*(len(data_t_temp))+k-int(t[i][a][0])]))\n",
    "            for k in range(len(lightcurve_length)-len(data_m[j])): # This part was used if the length of data is less than 185\n",
    "                data_m[j].append(data_m[j][-1])\n",
    "                data_m_err[j].append(np.sqrt(data_m_err[j][-1]))\n",
    "\n",
    "        lightcurve_length = lightcurve_length + (t[i][a][0] - lightcurve_length_before) # Time list\n",
    "\n",
    "        #print('data_m[0]=',np.array(data_m[0]).shape)\n",
    "\n",
    "        data_temp.append(lightcurve_length)\n",
    "        for j in range(len(Filter)):\n",
    "            data_temp.append(data_m[j])\n",
    "        for j in range(len(Filter)):\n",
    "            data_temp.append(data_m_err[j])\n",
    "        data.append([data_temp[j] for j in range(len(data_temp))])\n",
    "        \n",
    "        # Plot out the data input\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid()\n",
    "        for j in range(len(Filter)):\n",
    "            #plt.scatter(lightcurve_length, data_m[j], color=color[j], s=0.3)\n",
    "            plt.errorbar(lightcurve_length, data_m[j], yerr=data_m_err[j], fmt='.', color=color[j], lw=0.8)\n",
    "        plt.show()\n",
    "\n",
    "    data = np.array(data, dtype=object)\n",
    "    print('data shape=', data.shape)\n",
    "    \n",
    "    return data, data_mean, data_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filter_wavelength_JMC = [3.663, 4.361, 5.448, 6.407, 7.980]\n",
    "color = ['darkviolet', 'royalblue', 'seagreen', 'crimson', 'maroon', 'k']\n",
    "data_JMC, mean_JMC, range_JMC = GP_interpolate(t_JMC, m_JMC, m_err_JMC, Type_JMC, Filter_JMC, Filter_wavelength_JMC, filename_JMC, color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filter_wavelength_SDSS_p = [3.542, 4.724, 6.202, 7.759] # From Spanish Virtual Observatory\n",
    "color = ['darkviolet', 'seagreen', 'crimson', 'maroon', 'k']\n",
    "data_SDSS_p, mean_SDSS_p, range_SDSS_p = GP_interpolate(t_SDSS_p, m_SDSS_p, m_err_SDSS_p, Type_SDSS_p, Filter_SDSS_p, Filter_wavelength_SDSS_p, filename_SDSS_p, color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filter_wavelength_SDSS = [3.608, 4.671, 6.141, 7.457] # From Spanish Virtual Observatory\n",
    "color = ['darkviolet', 'seagreen', 'crimson', 'maroon', 'k']\n",
    "data_SDSS, mean_SDSS, range_SDSS = GP_interpolate(t_SDSS, m_SDSS, m_err_SDSS, Type_SDSS, Filter_SDSS, Filter_wavelength_SDSS, filename_SDSS, color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e765ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Conversion from SDSS_p to SDSS\n",
    "\n",
    "Equations from Andrew Pickles\n",
    "Las Cumbres Observatory Global Telescope, Santa Barbara, CA\n",
    "The 2010 STScI Calibration Workshop\n",
    "'''\n",
    "\n",
    "def unnormalization(data, mean, rang):\n",
    "    for i in range(data.shape[0]):\n",
    "        data[i,:,:] = data[i,:,:]*rang[i] + mean[i]\n",
    "    return data\n",
    "\n",
    "data_SDSS_p = unnormalization(data_SDSS_p, mean_SDSS_p, range_SDSS_p)\n",
    "data_JMC = unnormalization(data_JMC, mean_JMC, range_JMC)\n",
    "\n",
    "print(data_SDSS_p[0,1,:])\n",
    "\n",
    "def renormalization(i, data):\n",
    "    y = []\n",
    "    \n",
    "    for j in range(data.shape[1]):\n",
    "        for k in range(data.shape[2]):\n",
    "            y.append(data[i,j,k])\n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    y_range = np.max(y) - np.min(y)\n",
    "    \n",
    "    return data[i,:,:]\n",
    "    \n",
    "    \n",
    "\n",
    "data_SDSS_temp = np.zeros((data_SDSS_p.shape))\n",
    "\n",
    "for i in range(data_SDSS_p.shape[0]):\n",
    "    \n",
    "    data_SDSS_temp[i,0,:] =         data_SDSS_p[i,0,:]\n",
    "    data_SDSS_temp[i,1,:] = 0.037 + data_SDSS_p[i,1,:] + 0.060 * (data_SDSS_p[i,1,:] - data_SDSS_p[i,2,:] - 0.53)\n",
    "    data_SDSS_temp[i,2,:] = 0.010 + data_SDSS_p[i,2,:] + 0.035 * (data_SDSS_p[i,2,:] - data_SDSS_p[i,3,:] - 0.21)\n",
    "    data_SDSS_temp[i,3,:] =       + data_SDSS_p[i,3,:] + 0.041 * (data_SDSS_p[i,2,:] - data_SDSS_p[i,3,:] - 0.21)\n",
    "    \n",
    "    \n",
    "    y = []\n",
    "    \n",
    "    for j in range(data_SDSS_temp.shape[1]):\n",
    "        for k in range(data_SDSS_temp.shape[2]):\n",
    "            y.append(data_SDSS_temp[i,j,k])\n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    y_range = np\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51284bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
